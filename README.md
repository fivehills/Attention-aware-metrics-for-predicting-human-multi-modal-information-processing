# Attention-aware-metrics-for-predicting-human-multi-modal-information-processing
This project seeks to advance cognitive science and AI by merging these disciplines with computational fields, focusing on human language processing. It aims to evolve existing computational models - surprisal (expectation-based) and semantic similarity (memory-based) - by incorporating contextual information, a lesson learned from the shift from RNNs to Transformers in deep learning. Recognizing the multi-modal nature of human communication, this research addresses the limitations of current models that focus predominantly on English and single-mode processing. The proposed work involves enhancing these models into context-aware computational measures, drawing inspiration from deep learning techniques. These refined models aim to more accurately predict how humans process information across multiple modalities, including language and vision, validated by eye-movement, EEG, and fMRI data. This groundbreaking approach, applicable across various languages, seeks to create unified computational measures for understanding language and multi-modal processing. The broader goal is to impact computational cognition research, shedding light on cognitive science, AI, and neuroscience. The outcomes of this project are poised to improve human-computer interaction, enhance educational approaches in language learning, and offer insights into the human brain, potentially benefiting society in areas such as healthcare, communication, culture, and accessibility.
